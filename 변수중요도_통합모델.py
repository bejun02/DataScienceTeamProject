import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ
print("Loading data...")
king_df = pd.read_csv('King_County_Sold.csv', skiprows=[1])
pierce_df = pd.read_csv('Pierce_County_Sold.csv', skiprows=[1])

king_df['COUNTY'] = 'King'
pierce_df['COUNTY'] = 'Pierce'

df = pd.concat([king_df, pierce_df], ignore_index=True)

# í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
df = df[['PRICE', 'SQUARE FEET', 'BEDS', 'BATHS', 'YEAR BUILT', 
         'PROPERTY TYPE', 'COUNTY', 'LATITUDE', 'LONGITUDE']].dropna()

print(f"Total data: {len(df)}")

# ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜
from math import radians, sin, cos, sqrt, asin

def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return R * c

# ì‹œì• í‹€, ë²¨ë·° ì¢Œí‘œ
SEATTLE_COORDS = (47.6062, -122.3321)
BELLEVUE_COORDS = (47.6101, -122.2015)

print("Calculating distances...")
df['dist_seattle'] = df.apply(
    lambda row: haversine(row['LATITUDE'], row['LONGITUDE'], 
                          SEATTLE_COORDS[0], SEATTLE_COORDS[1]), axis=1)
df['dist_bellevue'] = df.apply(
    lambda row: haversine(row['LATITUDE'], row['LONGITUDE'], 
                          BELLEVUE_COORDS[0], BELLEVUE_COORDS[1]), axis=1)

# ë”ë¯¸ ë³€ìˆ˜ ìƒì„±
df['COUNTY_King'] = (df['COUNTY'] == 'King').astype(int)

# PROPERTY TYPE ë”ë¯¸ ë³€ìˆ˜ (Single Familyê°€ ê¸°ì¤€, Townhouse/Condoë§Œ ë”ë¯¸)
df['TYPE_Townhouse'] = (df['PROPERTY TYPE'] == 'Townhouse').astype(int)
df['TYPE_Condo'] = (df['PROPERTY TYPE'] == 'Condo/Co-op').astype(int)

# íŠ¹ì„± ì¤€ë¹„ (í†µí•© ëª¨ë¸: 9ë³€ìˆ˜)
feature_cols = ['SQUARE FEET', 'BEDS', 'BATHS', 'YEAR BUILT', 
                'COUNTY_King', 'TYPE_Townhouse', 'TYPE_Condo',
                'dist_seattle', 'dist_bellevue']

X = df[feature_cols]
y = df['PRICE']

print(f"\nModel features: {len(feature_cols)}")
print(f"  Basic: SQFT, BEDS, BATHS, YEAR_BUILT")
print(f"  County: COUNTY_King")
print(f"  Type: TYPE_Townhouse, TYPE_Condo")
print(f"  Location: dist_seattle, dist_bellevue")

# Train-Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest í•™ìŠµ
print("\nğŸŒ² Training Random Forest...")
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# ì„±ëŠ¥ í‰ê°€
train_score = rf_model.score(X_train, y_train)
test_score = rf_model.score(X_test, y_test)

print(f"  Train RÂ²: {train_score:.4f}")
print(f"  Test RÂ²: {test_score:.4f}")

# Feature Importance ì¶”ì¶œ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
importances = rf_model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': importances
}).sort_values('Importance', ascending=True)  # ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ (barhì—ì„œ ìœ„ê°€ ì‘ì€ ê°’)

print("\nğŸ“Š Feature Importance (MDI):")
# ì¶œë ¥ì€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ
for idx, row in feature_importance_df.sort_values('Importance', ascending=False).iterrows():
    print(f"  {row['Feature']}: {row['Importance']*100:.1f}%")

# í•œê¸€ ë¼ë²¨ ë§¤í•‘
label_mapping = {
    'SQUARE FEET': 'SQFT',
    'BEDS': 'BEDS',
    'BATHS': 'BATHS',
    'YEAR BUILT': 'YEAR_BUILT',
    'COUNTY_King': 'COUNTY_King',
    'TYPE_Townhouse': 'TYPE_Townhouse',
    'TYPE_Condo': 'TYPE_Condo',
    'dist_seattle': 'dist_seattle',
    'dist_bellevue': 'dist_bellevue'
}

feature_importance_df['Feature_Label'] = feature_importance_df['Feature'].map(label_mapping)

# ì‹œê°í™”
fig, ax = plt.subplots(figsize=(12, 8))

# ìƒ‰ìƒ ì§€ì • (ì¹´í…Œê³ ë¦¬ë³„)
colors = []
for feat in feature_importance_df['Feature']:
    if feat == 'SQUARE FEET':
        colors.append('#FF8C00')  # ì£¼í™© - ë©´ì 
    elif feat in ['dist_bellevue', 'dist_seattle']:
        colors.append('#2E86AB')  # íŒŒë‘ - ìœ„ì¹˜
    elif feat == 'COUNTY_King':
        colors.append('#9370DB')  # ë³´ë¼ - ì¹´ìš´í‹°
    elif 'TYPE' in feat:
        colors.append('#32CD32')  # ì´ˆë¡ - ìœ í˜•
    else:
        colors.append('#708090')  # íšŒìƒ‰ - ê¸°íƒ€

# ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ (ì¤‘ìš”ë„ ë†’ì€ ê²ƒì´ ìœ„ë¡œ)
bars = ax.barh(range(len(feature_importance_df)), 
               feature_importance_df['Importance'],
               color=colors, edgecolor='black', linewidth=1.5, alpha=0.85)

# Yì¶• ë¼ë²¨ ì„¤ì • (ë‚´ë¦¼ì°¨ìˆœ)
ax.set_yticks(range(len(feature_importance_df)))
ax.set_yticklabels(feature_importance_df['Feature_Label'])

# ê°’ ë¼ë²¨ ì¶”ê°€
for i, (idx, row) in enumerate(feature_importance_df.iterrows()):
    ax.text(row['Importance'] + 0.01, i, 
            f"{row['Importance']*100:.1f}%", 
            va='center', fontsize=11, fontweight='bold')

ax.set_xlabel('Feature Importance', fontsize=13, fontweight='bold')
ax.set_ylabel('')
ax.set_title('Random Forest Feature Importance Analysis (Unified Model)\n"SQFT accounts for over 50% of total importance"', 
             fontsize=15, fontweight='bold', pad=20)
ax.set_xlim(0, max(feature_importance_df['Importance']) * 1.15)

# ê·¸ë¦¬ë“œ ì¶”ê°€
ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)

# ë²”ë¡€ ì¶”ê°€
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='#FF8C00', label='Area (SQFT)', edgecolor='black'),
    Patch(facecolor='#2E86AB', label='Location (Distance)', edgecolor='black'),
    Patch(facecolor='#9370DB', label='County', edgecolor='black'),
    Patch(facecolor='#32CD32', label='Property Type', edgecolor='black'),
    Patch(facecolor='#708090', label='Others', edgecolor='black')
]
ax.legend(handles=legend_elements, loc='lower right', fontsize=11, framealpha=0.9)

# ìƒìœ„ 75% ì°¨ì§€í•˜ëŠ” ë³€ìˆ˜ í‘œì‹œ
cumsum = feature_importance_df.sort_values('Importance', ascending=False)['Importance'].cumsum()
top_75_count = (cumsum <= 0.75).sum()
if top_75_count == 0:
    top_75_count = 1

ax.axhline(y=len(feature_importance_df) - top_75_count - 0.5, 
           color='red', linestyle='--', linewidth=2, alpha=0.6)
ax.text(0.45, len(feature_importance_df) - top_75_count - 0.3, 
        f'â† Top {top_75_count} variables account for 75%+', 
        fontsize=11, color='red', fontweight='bold', va='bottom')

plt.tight_layout()
plt.savefig('ë³€ìˆ˜ì¤‘ìš”ë„_í†µí•©ëª¨ë¸.png', dpi=300, bbox_inches='tight')
print("\nâœ… Graph saved: ë³€ìˆ˜ì¤‘ìš”ë„_í†µí•©ëª¨ë¸.png")

# ìƒìœ„ ë³€ìˆ˜ ëˆ„ì  ë¹„ìœ¨
print("\nğŸ“ˆ Cumulative Feature Importance:")
cumsum_pct = 0
for i, (idx, row) in enumerate(feature_importance_df.sort_values('Importance', ascending=False).iterrows()):
    cumsum_pct += row['Importance']
    print(f"  Top {i+1}: {cumsum_pct*100:.1f}%")
    if cumsum_pct >= 0.75:
        print(f"  â†’ Top {i+1} variables account for 75%")
        break

plt.show()
